# DATA ANALYSIS & PREPROCESSING
import pandas as pd

# Load the data
file_path = '/content/data.tsv' 
df = pd.read_csv(file_path, sep='\t', header=None, names=['word', 'pos', 'label'])

# Handling missing or incorrect data
df.dropna(inplace=True)

# Display the first few rows of the raw dataset for analysis
print("Raw Dataset:\n", df.head())

# Grouping words, POS tags, and labels into sentences
sentences = []
sentence_words = []
sentence_pos = []
sentence_labels = []

for _, row in df.iterrows():
    word, pos, label = row['word'], row['pos'], row['label']
    
    if word == '।':  # End of a sentence
        sentences.append({
            'sentence': sentence_words,
            'pos': sentence_pos,
            'labels': sentence_labels
        })
        sentence_words, sentence_pos, sentence_labels = [], [], []
    else:
        sentence_words.append(word)
        sentence_pos.append(pos)
        sentence_labels.append(label)

# Add the last sentence
if sentence_words:
    sentences.append({
        'sentence': sentence_words,
        'pos': sentence_pos,
        'labels': sentence_labels
    })

sentences_df = pd.DataFrame(sentences)
print("Preprocessed Dataset:\n", sentences_df.head())

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Initialize tokenizers
word_tokenizer = Tokenizer(lower=False, oov_token="<OOV>")
label_tokenizer = Tokenizer(lower=False, oov_token="<OOV>")

# Fit tokenizers on the dataset
word_tokenizer.fit_on_texts(sentences_df['sentence'])
label_tokenizer.fit_on_texts(sentences_df['labels'])

# Convert words and labels to sequences
X = word_tokenizer.texts_to_sequences(sentences_df['sentence'])
y = label_tokenizer.texts_to_sequences(sentences_df['labels'])

# Find the maximum sequence length in the dataset
max_len = max(len(x) for x in X)

# Pad sequences to ensure uniform length
X_padded = pad_sequences(X, maxlen=max_len, padding='post')
y_padded = pad_sequences(y, maxlen=max_len, padding='post')

# Convert labels to categorical format (one-hot encoding)
y_padded = np.array([to_categorical(i, num_classes=len(label_tokenizer.word_index) + 1) for i in y_padded])

# Display the padded sequences for analysis
print("Padded Sequences (X):\n", X_padded)
print("Padded Sequences (y):\n", y_padded)

from sklearn.model_selection import train_test_split

# Split the dataset
X_train, X_temp, y_train, y_temp = train_test_split(X_padded, y_padded, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Display the sizes of each split for analysis
print("Training Set Size:", len(X_train))
print("Validation Set Size:", len(X_val))
print("Test Set Size:", len(X_test))

# MODEL DEVELOPMENT
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense
# from tensorflow_addons.layers import CRF

# Model hyperparameters
input_dim = len(word_tokenizer.word_index) + 1  # Vocabulary size
output_dim = 64  # Embedding output size
input_length = max_len  # Length of input sequences

# Model definition
input_layer = Input(shape=(input_length,))
embedding_layer = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length)(input_layer)
bilstm_layer = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(embedding_layer)
dense_layer = TimeDistributed(Dense(23, activation="relu"))(bilstm_layer)

model = Model(input_layer, dense_layer)
METRICS = [
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall'),
      tf.keras.metrics.AUC(name='auc')
]
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=METRICS)

model.summary()

# Convert y_train and y_val to integers
y_train = y_train.astype('int32')
y_val = y_val.astype('int32')

# Train the model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    batch_size=32, epochs=5, verbose=1
)

# MODEL EVALUATION
# Evaluate the model on the test set
test_loss, test_accuracy, test_precision, test_recall, test_auc = model.evaluate(X_test, y_test)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Precision:", test_precision)
print("Test Recall:", test_recall)
print("Test AUC:", test_auc)

from sklearn.metrics import classification_report, accuracy_score

# Predict the labels for the test set
y_pred = model.predict(X_test)

# Convert predictions and ground truth back to label indices
y_pred = np.argmax(y_pred, axis=-1)
y_test_true = np.argmax(y_test, axis=-1)

# Flatten the results for comparison
y_pred_flat = [pred for sublist in y_pred for pred in sublist]
y_test_true_flat = [true for sublist in y_test_true for true in sublist]

# Filter out padding (usually represented by index 0)
y_pred_flat = [pred for pred, true in zip(y_pred_flat, y_test_true_flat) if true != 0]
y_test_true_flat = [true for true in y_test_true_flat if true != 0]

# Generate and print the classification report
print("Accuracy:", accuracy_score(y_test_true_flat, y_pred_flat))
print(classification_report(y_test_true_flat, y_pred_flat))

# MODEL DEPLOYMENT
model.save('pos_ner_model.h5') 
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow_addons.layers import CRF
import numpy as np

# Load the model
model = load_model('pos_ner_model.h5')

# Define a function for preprocessing new data (you can adjust this to fit your preprocessing pipeline)
def preprocess_input(texts, word_tokenizer, max_len):
    # Tokenize the input texts
    sequences = word_tokenizer.texts_to_sequences(texts)
    
    # Pad sequences to match the input length expected by the model
    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')
    
    return padded_sequences

# Example new data
new_sentences = ["শনিবার রাতে পটুয়াখালী সফর করেন।"]

# Preprocess the new data
# Assuming `word_tokenizer` and `max_len` are predefined based on training data
preprocessed_input = preprocess_input(new_sentences, word_tokenizer, max_len)

# Make predictions
predictions = model.predict(preprocessed_input)

# Decode the predictions (this depends on your label tokenizer)
def decode_predictions(preds, label_tokenizer):
    pred_labels = np.argmax(preds, axis=-1)
    decoded_labels = label_tokenizer.sequences_to_texts(pred_labels)
    return decoded_labels

# Decode the predicted labels
predicted_labels = decode_predictions(predictions, label_tokenizer)

# Display the results
for sentence, labels in zip(new_sentences, predicted_labels):
    print(f"Sentence: {sentence}")
    print(f"Predicted Labels: {labels}")
